import{a as e,c as p,d as s,o as i}from"./app-D5tjPIRP.js";const o="/molihua-wiki/assets/image-DWHTtdCs.png",n="/molihua-wiki/assets/diagram-CeE2ptSS.png",t={};function r(c,a){return i(),p("div",null,[...a[0]||(a[0]=[s('<h2 id="一、虚拟文件系统vfs" tabindex="-1"><a class="header-anchor" href="#一、虚拟文件系统vfs"><span>一、虚拟文件系统VFS</span></a></h2><p>因为有很多种文件系统（磁盘、内存、网络），为了让上层应用方便使用，引入一个中间层VFS，上层只需要知道VFS的接口，不需要知道底层FS的接口。</p><p><strong>Linux：一切皆文件</strong></p><p>不管文件目录，还是socket、管道等，都是文件形式存储的</p><p>在内核中，会为每个文件分配两个数据结构</p><ul><li><p>inode 索引节点：记录文件元信息（编号、大小、访问权限等），作为唯一标识，也会存储在硬盘中</p></li><li><p>dentry 目录项：记录文件名字，inode指针，与其他目录项的层级结构，它是内核维护的一个数据结构，不会存储在硬盘中，而是缓存在内存中。目录项的主要功能是将一个文件名映射到文件系统的inode。</p></li></ul><p><strong>目录项与目录？</strong></p><ul><li>目录本质上是一个特殊类型的文件，其数据结构包含了对目录下文件和子目录的引用。目录包含目录项（directory entries）的列表。也就是说dentry实际上还是有持久化的，只不过往往会被内核在内存中做一个缓冲cache。</li></ul><p>用户进程读写文件的单位是字节，os读写文件的单位是数据块，文件系统的工作是屏蔽掉这种差异。</p><h2 id="二、文件存储" tabindex="-1"><a class="header-anchor" href="#二、文件存储"><span>二、文件存储</span></a></h2><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><blockquote><h3 id="unix文件存储形式" tabindex="-1"><a class="header-anchor" href="#unix文件存储形式"><span>Unix文件存储形式</span></a></h3><p>文件头inode中带有13个索引项</p><p>1-10个指向文件数据块</p><p>11指向一级索引块</p><p>12指向二级索引块</p><p>13指向三级索引块</p><p><strong>对于小文件可以直接查找到文件，大文件可以通过索引的方式查询索引块</strong></p></blockquote><blockquote><h3 id="软链接和硬链接" tabindex="-1"><a class="header-anchor" href="#软链接和硬链接"><span>软链接和硬链接</span></a></h3><p>两者都是一个目录项，保存在目录文件中。</p><p>硬链接：直接指向目标inode，每个文件系统都有自己的inode，所以硬链接不能跨文件系统，所以只有删除全部硬链接和源文件，这个文件才会删除。</p><p>软链接：相当于创建一个新的文件，有自己的inode，文件内容是目标文件的路径，可以跨文件系统，所以源文件删除，文件就没了，但软链接文件还会存在。</p></blockquote><h2 id="三、文件i-o" tabindex="-1"><a class="header-anchor" href="#三、文件i-o"><span>三、文件I/O</span></a></h2><p>文件IO有很多种</p><h3 id="_1-缓冲-io-buffer-io" tabindex="-1"><a class="header-anchor" href="#_1-缓冲-io-buffer-io"><span>1. 缓冲 IO （Buffer IO）</span></a></h3><p>数据先从磁盘复制到内核空间的缓冲区（page cache），然后从内核空间缓冲区复制到应用程序的地址空间。</p><p>实际的读写都是对page cache做的，而实际操作到文件中，由操作系统来做调度。</p><p><strong>读操作</strong>：直接读cache，如果没有cache就从磁盘读到cache中。</p><p><strong>写操作</strong>：都是先写到cache中，如果制定fsync，则立马刷到盘中然后返回，否则就立即返回，是否刷盘由操作系统决定。</p><p><strong>优点</strong>：减少刷盘次数，隔离用户空间和内存空间</p><p><strong>缺点</strong>：虽然可以利用DMA，但是需要把数据在用户空间、内核空间、磁盘上拷贝，对CPU、内存开销大。</p><p><code>DMA：避免让cpu阻塞在大量数据搬运的操作中，DMA = Direct Memory Access直接内存存取，可以不经过CPU，实现数据复制的任务。</code></p><blockquote><h3 id="mmap和常规io的差别" tabindex="-1"><a class="header-anchor" href="#mmap和常规io的差别"><span>mmap和常规IO的差别？</span></a></h3><p>常规IO会先切换到内核态，定位文件信息，找到inode，查找inode有没有在page cache里面，不在的话，通过inode定位磁盘地址，将数据<strong>复制</strong>到page cache，之后再发起读page cache的过程，将数据从内核空间<strong>复制</strong>到用户空间。</p><p>mmap则是直接在进程的虚拟地址空间中，寻找一段连续的空闲的地址，然后切换到内核态，调用内核函数mmap，建立页表，实现文件地址和虚拟地址的映射关系。然后进程发起对这片映射空间的访问，引起缺页异常，实现文件内容到主存的<strong>复制</strong></p><p>mmap实际上可以简单理解为对用户空间的页表做了操作，使其可以映射到内核空间去。</p></blockquote><blockquote><h3 id="page-cache是什么" tabindex="-1"><a class="header-anchor" href="#page-cache是什么"><span>Page Cache是什么？</span></a></h3><p>是os提供的一个机制，将文件缓存在内核空间中，减少文件IO操作，提高整体的性能。当读取文件时，先看看page cache中有没有，如果有就直接读取，没有的话就会读取文件到cache中。</p></blockquote><h3 id="_2-直接-io-direct-io" tabindex="-1"><a class="header-anchor" href="#_2-直接-io-direct-io"><span>2. 直接 IO （Direct IO）</span></a></h3><p>一般来说缓冲IO就已经是很好用了，但有的应用（比如数据库），认为自己的应用层缓存已经做的很不错了，可以不需要缓冲IO的cache也能有比较优秀的读性能，出于节省内存和CPU，就会使用直接IO。</p><p>当应用程序需要直接访问文件而不经过page cache的时候，它打开文件的时候需要指定<strong>O_DIRECT</strong>标识符。</p><p><strong>优点：</strong>减少拷贝次数</p><p><strong>缺点：</strong>读写性能跟应用层缓存关系很大。</p><h3 id="_3-bio-nio" tabindex="-1"><a class="header-anchor" href="#_3-bio-nio"><span>3. BIO - NIO</span></a></h3><p><strong>BIO：</strong>当用户程序执行read()时，线程会阻塞到内核把数据准备好，并把数据从内核缓冲区复制到用户缓冲区，这时read才会返回。</p><p>等待的是两个过程：内核数据准备 + 数据复制到用户态</p><p><strong>NIO：</strong>内核数据没准备好时会直接返回，告知没准备好，让用户程序轮询read，直到内核数据准备好，再阻塞到数据复制好。</p><p>非阻塞的过程是内核数据准备的过程。</p><p>站在应用程序的角度，一直轮询read肯定不好，所以就有了<strong>IO多路复用技术</strong>，比如select、poll，通过IO事件分发，如果没有数据准备好，当前线程就会阻塞，当有数据准备好时，就通知唤醒线程去做处理。</p><p>看起来似乎和NIO没啥区别，但是有个巨大优点：可以在一个线程中处理多个socket的IO请求。</p><p>以上提到的都是同步IO，都是需要等待数据拷贝的过程的。</p><blockquote><h3 id="网络通信为什么需要io多路复用" tabindex="-1"><a class="header-anchor" href="#网络通信为什么需要io多路复用"><span>网络通信为什么需要IO多路复用？</span></a></h3><p>因为网络通信主要是基于socket模型来进行的：</p><ul><li><p>服务端 </p><ol><li><p>bind(): 服务端会创建一个Socket并绑定到一个IP地址和端口</p></li><li><p>listen(): 监听这个ip:port</p></li><li><p>accept(): 从内核获取客户端的连接，如果没有连接，就会阻塞住</p></li></ol></li><li><p>客户端 </p><ol><li>connect(): 发起三次握手连接</li></ol></li></ul><p>在TCP连接的过程中，会有半连接和全连接两个队列，如果全连接队列不是空的，accept就会从队列中拿出一个socket返回给应用程序，这之后双方就可以开始read、write</p><p>而在Linux中一切皆文件，<strong>socket也是文件，也就会对应一个文件描述符fd</strong>，socket文件中包含了指向发送和接收的数据包的指针。</p><h3 id="同步阻塞bio" tabindex="-1"><a class="header-anchor" href="#同步阻塞bio"><span>同步阻塞BIO：</span></a></h3><p>每次只能处理一个请求，无法并发，就算是把处理的逻辑进行多线程处理，开销也很大</p><h3 id="同步非阻塞nio" tabindex="-1"><a class="header-anchor" href="#同步非阻塞nio"><span>同步非阻塞NIO：</span></a></h3><p>每次先尝试accept，有连接就加入一个fds集合，没连接就立马进行下一轮循环逻辑（导致CPU忙轮询）。每轮都会遍历一遍fds，查看有没有可以读写的数据。<strong>IO多路复用就属于NIO的一种。</strong></p><h3 id="io多路复用" tabindex="-1"><a class="header-anchor" href="#io多路复用"><span>IO多路复用：</span></a></h3><p>服务端采用单线程通过 <code>select/poll/epoll</code> 等系统调用获取 fd 列表，遍历有事件的 fd 进行 <code>accept/recv/send</code> ，使其能支持更多的并发连接请求。</p><ul><li><p>select/poll：</p></li><li><p>将已连接的 Socket 都放到一个fds集合，然后把fds拷贝到内核，内核遍历fds，标记可读可写的fd，然后再将fds拷贝回用户态，用户态遍历fds，找到可以读写的socket。</p></li><li><p>2次拷贝、2次遍历</p></li><li><p>select的fds用的是BitsMap，poll用的是链表，本质上没有太大差别。</p></li><li><p>select的最大连接数大小有 <code>FD_SETSIZE</code> 来限制，poll使用的链表没有最大连接数的限制</p></li><li><p>当客户端越多，也就是 Socket 集合越大，Socket 集合的遍历和拷贝会带来很大的开销</p></li><li><p>epoll：</p><ol><li><p>epoll 使用<strong>事件驱动</strong>的机制，内核里<strong>维护了一个链表来记录就绪事件，当某个 socket 有事件发生时，通过回调函数</strong>内核会将其加入到这个就绪事件列表中，当用户调用<code>epoll_wait()</code>函数时，只会返回有事件发生的文件描述符的个数。不需要像 select/poll 那样轮询扫描整个集合。</p></li><li><p>epoll 在内核里使用红黑树来跟踪进程所有待检测的文件描述字，把需要监控的 socket 通过 <code>epoll_ctl()</code> 函数加入内核中的红黑树里，红黑树的增删改复杂度为O(logN)。不需要像 select/poll 在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。</p></li></ol></li></ul></blockquote><h3 id="_4-aio" tabindex="-1"><a class="header-anchor" href="#_4-aio"><span>4. AIO</span></a></h3><p>真正异步的IO，上面提到的两个过程都不用等待。</p><p>具体体现就是，BIO的过程中，线程不需要阻塞等待，而是可以立即返回，等待内核拷贝数据完成之后，再来通知线程。</p><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><p>IO有非常非常多分类的，不同的所谓NIO BIO DirectIO等，并不是平行的一些概念，具体还是要根据一些内容和特点来分类。</p><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="四、零拷贝" tabindex="-1"><a class="header-anchor" href="#四、零拷贝"><span>四、零拷贝</span></a></h2><p>零：用户态和内核态间 0 次拷贝。因为这种拷贝不能通过DMA，只能由CPU拷贝，占用大量CPU资源。、</p><h3 id="以文件传输为例" tabindex="-1"><a class="header-anchor" href="#以文件传输为例"><span>以文件传输为例</span></a></h3><p>文件传输涉及两个步骤：</p><ol><li><p>read：从本地把需要发送的文件读出来</p></li><li><p>write：把文件数据写到IO设备中，比如网卡</p></li></ol><h3 id="_1-传统传输方式" tabindex="-1"><a class="header-anchor" href="#_1-传统传输方式"><span>1. 传统传输方式：</span></a></h3><p>主要有四个拷贝过程，2次系统调用（4次CPU上下文切换）：</p><p>read：阻塞、系统调用sys_read</p><p>write：阻塞、系统调用sys_write</p><p>一般来说，不需要对数据做加工处理的场景，是没有必要把数据拷贝到用户态的。这种拷贝只能CPU来进行，性能不行。</p><h3 id="_2-mmap-write" tabindex="-1"><a class="header-anchor" href="#_2-mmap-write"><span>2. mmap + write</span></a></h3><p>使用mmap来加速read过程，mmap也会做系统调用</p><p>read：</p><p>write：阻塞、系统调用sys_write</p><p>mmap可以节省一次拷贝，一共3次拷贝，但还是有两次系统调用</p><h3 id="_3-sendfile" tabindex="-1"><a class="header-anchor" href="#_3-sendfile"><span>3. sendfile</span></a></h3><p>将read 和write 结合为一次系统调用</p><p>节省了一次系统调用。</p><p>但这还是有一次CPU拷贝，所以linux高版本支持了SG-DMA技术，可以把CPU拷贝的那一次节省。真正实现了零拷贝。一共两次DMA拷贝。</p><h2 id="贡献者" tabindex="-1"><a class="header-anchor" href="#贡献者"><span>贡献者</span></a></h2><div class="contributors-list" style="display:flex;gap:20px;flex-wrap:wrap;margin-top:20px;"><div style="text-align:center;"><img src="https://avatars.githubusercontent.com/u/94302726?v=4" alt="魏知乐" style="width:80px;border-radius:50%;"><p style="margin-top:8px;"><a href="https://github.com/spaceluke" target="_blank">魏知乐</a></p></div></div>',66)])])}const d=e(t,[["render",r]]),h=JSON.parse('{"path":"/ComputerFundamentals/gcr4s2zn/","title":"文件系统和IO","lang":"zh-CN","frontmatter":{"title":"文件系统和IO","createTime":"2025/10/05 23:08:26","permalink":"/ComputerFundamentals/gcr4s2zn/"},"readingTime":{"minutes":9.33,"words":2799},"git":{"createdTime":1759912112000,"updatedTime":1759912112000},"filePathRelative":"notes/ComputerFundamentals/文件系统和IO.md","headers":[]}');export{d as comp,h as data};
