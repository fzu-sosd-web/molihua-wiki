---
title: 基础内容概述
createTime: 2025/10/19 15:42:04
permalink: /LearningRoute/cet5kng4/
---
# 学前须知

[ 什么是人工智能？](https://ni3hb10ebs9.feishu.cn/docx/MexDdtomJoA49fx2iuWckQwXnaf?from=from_copylink)

# 软件安装及环境配置

> **从这开始才是AI-NLP入门指南的主体部分**

* **电脑配置**：一般建议有NVIDIA显卡以支持CUDA加速。到NLP部分可以考虑云服务器。

* **软件安装**：

  * **Anaconda or miniconda or uv**：用于包管理和环境配置，简化库的安装。虚拟环境的使用是必不可少的。（个人推荐anaconda，除了内存大没什么缺点）

  * **PyCharm / Vscode / Cursor**：开发工具。喜欢哪个用哪个。

  * **Python**：建议安装>=3.7版本。

  * **vpn:&#x20;**&#x8BBF;问外网会用上。需要连接外网的vpn，如果使用的是实验室的服务器，需要使用校园网or[福大vpn](https://wxb.fzu.edu.cn/info/1011/1191.htm)

  * 可参考[Anaconda安装+PyCharm安装和基本使用，Python编程环境安装](https://www.bilibili.com/video/BV1Vu411Y7gL/?spm_id_from=333.337.search-card.all.click\&vd_source=b98cb49910094c6df38524c7abf9ff10)

  * pycharm如果想用专业版可以申请学生认证，也可以看看pdd（当然pdd里其他工具如XSHELL、XFTP也能用上）

  * 建议下载anaconda用国内的镜像下比如**https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/**

  * [什么是python uv，如何在windows上安装uv，基础的用法有哪些？](https://blog.csdn.net/xiezhipu/article/details/145638765?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522475a980d3161a0d1e145e2f7a6add706%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D\&request_id=475a980d3161a0d1e145e2f7a6add706\&biz_id=0\&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-145638765-null-null.142^v102^pc_search_result_base3\&utm_term=uv\&spm=1018.2226.3001.4187)（uv在开发比较好用）



# Python基础

#### **1. 介绍/要求**

Python 是深度学习和 NLP 领域最常用的编程语言，学习时需掌握**面向对象编程、数据处理、常见库的使用**等核心技能。

* **重点掌握面向对象编程（OOP）**

  * 学习 **类（class）** 和 **对象（object）** 的概念，掌握如何创建类和对象。

  * 重点理解 **类的继承、多态、封装**，以及**类之间的关系（如组合、关联、继承）**。

  * 熟悉 `__init__` 构造方法、类方法 `@classmethod`、静态方法 `@staticmethod`、魔法方法 `__str__` 等。

* **基本编程能力**

  * 具备编写 Python 代码的基础，包括变量、数据类型、控制流（循环、条件语句）、函数、错误处理（try-except）。

  * 了解 Python 的常见数据结构：列表（list）、字典（dict）、集合（set）、元组（tuple），以及它们的操作方法。

  * 掌握 Python 内置函数，如 `map`、`filter`、`zip`、`enumerate`，提高代码效率。

* **正则表达式（re 模块）**：在文本处理、NLP 任务中常用。

* **函数式编程**：学习 `lambda`、`map`、`reduce`、`filter`，提高代码简洁性。

* **模块与包管理**

  * 学习如何使用 `import` 导入 Python 标准库和第三方库。

  * 掌握 `pip` 安装、管理 Python 库，以及虚拟环境 `venv` 或 `conda` 的使用。

* **爬虫相关内容不用看**

  * 学习深度学习和 NLP 任务时，爬虫（如 `requests`、`BeautifulSoup`、`Scrapy`）不是必须的内容。

  * 如果感兴趣，可以尝试爬取 NLP 训练数据，比如爬取新闻、社交媒体文本等。

### 学习参考

* [菜鸟教程](https://www.runoob.com/)

* [黑马程序员](https://space.bilibili.com/37974444)

* [尚硅谷](https://space.bilibili.com/302417610)

* [零基础入门学习Python](https://www.bilibili.com/video/BV1c4411e77t/?spm_id_from=333.337.search-card.all.click\&vd_source=b98cb49910094c6df38524c7abf9ff10)



# 数据科学入门

### **1. 介绍/要求**

在深度学习、NLP 任务以及数据科学竞赛（如 Kaggle，数学建模大赛）中，**数据处理和数据可视化**是至关重要的技能。高质量的数据预处理往往比模型选择更重要，而数据可视化可以帮助我们更好地理解数据、优化模型。

***

### **2. Pandas：数据处理**

Pandas 是 Python 最常用的数据分析库，能够高效处理**结构化数据**（如 CSV、Excel、数据库表等）。

* **学习 DataFrame（数据框）**

  * 创建 DataFrame（从字典、列表、NumPy 数组、CSV 文件等创建）。

  * 熟悉索引和列操作（`df.iloc`、`df.loc`）。

  * 进行数据筛选（条件查询、索引操作）。

* **数据清洗**

  * 处理缺失值（`df.fillna()` 填充、`df.dropna()` 删除）。

  * 处理重复值（`df.duplicated()` 检测，`df.drop_duplicates()` 删除）。

  * 数据格式转换（字符串、日期时间转换）。

* **数据分组与聚合**

  * 使用 `groupby()` 进行数据分组统计。

  * 计算均值、中位数、最大最小值等统计信息。

* **数据导入导出**

  * 读取和保存 CSV、JSON 文件（`pd.read_csv()`，`df.to_csv()`）。

  * 处理 Excel（`pd.read_excel()`）。

***

### **3. NumPy：数值计算**

NumPy 是**处理大规模数组和矩阵计算**的核心库，在深度学习和 NLP 任务中，很多数据处理都需要 NumPy 操作。

* **NumPy 数组（ndarray）**

  * 创建数组（`np.array()`、`np.zeros()`、`np.ones()`、`np.arange()`）。

  * 索引、切片（类似于 Python 列表，但更高效）。

  * 生成随机数，例如：&#x20;

  ```python
  import numpy as np
  np.random.randint(1, 100, size=100)  # 生成 100 个 1-100 之间的随机整数
  ```

* **数学函数**

  * 线性代数运算（`np.dot()` 矩阵乘法、`np.linalg.inv()` 逆矩阵）。

  * 统计函数（`np.mean()`、`np.std()`、`np.median()`）。

  * 广播机制：在不同形状的数组之间进行运算。

***

### **4. Matplotlib 与 Seaborn：数据可视化**

数据可视化是理解数据、调试模型的重要工具。

#### **4.1 Matplotlib**

Matplotlib 是 Python 最基础的绘图库，可以用于创建多种图表：

* **学习基本绘图**

  &#x20;包括且不限于：

  * 折线图（`plt.plot()`）

  * 柱状图（`plt.bar()`）

  * 散点图（`plt.scatter()`）

  * 直方图（`plt.hist()`）

* **可视化自定义**

  * 设置标题、坐标轴标签、图例。

  * 自定义颜色、样式、网格线。



#### **4.2 Seaborn**

Seaborn 在 Matplotlib 基础上进行了优化，提供更**美观、简洁**的统计图形：

* **常见图表**

  * 分布图（`sns.histplot()`、`sns.kdeplot()`）。

  * 盒须图（`sns.boxplot()`）—— 可视化数据分布及异常值。

  * 热力图（`sns.heatmap()`）—— 适用于相关性分析。

* **风格调整**

  * 设置主题（`sns.set_theme()`）。

  * 组合多个图表（如 `sns.pairplot()` 进行数据探索）。

### 学习参考

* 同上文。

* 也可以把文档复制粘贴给gpt让他教你

# PyTorch基础

**介绍/要求**

PyTorch 是当前主流的深度学习框架之一，具有 **动态图计算**、**易调试**、**强大的 GPU 支持** 等优点，被广泛用于计算机视觉（CV）、自然语言处理（NLP）等领域。
**建议学习阶段使用 PyTorch 版本低于 2.0（尤其是想学CV的）**，因为学习过程中可能会遇到 **老代码与 2.0 版本不兼容的问题**。当然，随着学习进度的推进，也要创建相应的环境，比如到大语言模型阶段，一般都常驻在2.0以上。

### **第一阶段：PyTorch 基础**

#### **（1） 张量（Tensor）操作**

* 张量是 PyTorch 的核心数据结构，类似于 NumPy 的多维数组，但支持 GPU 加速计算。

* 需要掌握的操作包括 **张量的创建、索引、切片、变形、运算** 等。

* 重点理解 **如何获取张量的转置** 以及 **如何改变张量的形状**，这些操作在构建深度学习模型时非常常见。

#### **（2） torchvision.transforms 进行数据预处理**

* `torchvision.transforms` 是 PyTorch 处理 **图像数据** 的重要工具，常用于数据增强和预处理。

* 关键操作包括 **调整尺寸、归一化、随机翻转** 等，以提高模型的泛化能力。

#### **（3） Dataset 与 Dataloader**

* PyTorch 通过 `Dataset` 和 `Dataloader` 进行数据加载和批量处理，提供 **自动化的数据迭代机制**，可以高效地训练神经网络。

* 需要掌握如何 **自定义 Dataset**，并进行 **数据预处理** 和 **批量加载**。

比如

```python
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# 生成数据
import torch
X = torch.randn(100, 10)  # 100 个样本，每个样本 10 维
y = torch.randint(0, 2, (100,))  # 100 个二分类标签

dataset = CustomDataset(X, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for batch_X, batch_y in dataloader:
    print(batch_X.shape, batch_y.shape)  # 输出 (32,10) (32,)

```

#### **（4） torch.device**

* PyTorch 允许使用 **GPU 加速计算**，可以通过 `torch.device` 在 **CPU 和 GPU 之间切换**。

* 了解如何 **将数据和模型放置到指定设备**，优化训练效率。

比如

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)
model = MyModel().to(device)
inputs, labels = inputs.to(device), labels.to(device)
```

#### **（5） 多层感知机（MLP）**

* 多层感知机（MLP）是最基础的神经网络结构，由 **输入层、隐藏层、输出层** 组成。

* 需要理解 **前向传播（Forward Propagation）** 和 **反向传播（Backward Propagation）** 计算过程，以及如何通过 **激活函数** 使模型具备非线性表达能力。

比如

```python
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = MLP(input_dim=10, hidden_dim=32, output_dim=2)

```

#### **（6） 损失函数与优化器**

* **损失函数** 用于衡量模型的预测结果与真实值的误差，不同任务使用不同的损失函数：&#x20;

  * **MSELoss**：用于回归任务，计算均方误差。

  * **CrossEntropyLoss**：用于分类任务，衡量预测类别的交叉熵损失。

* **优化器** 用于更新模型参数：&#x20;

  * **SGD（随机梯度下降）**：计算简单，但可能收敛较慢。

  * **Adam（自适应优化）**：能够自动调整学习率，提高训练速度。

#### **（7） 正则化**

* 正则化用于防止模型 **过拟合**，提升泛化能力。

* 主要包括：&#x20;

  * **L1 正则化**：鼓励参数稀疏，适用于特征选择任务。

  * **L2 正则化**（权重衰减）：防止模型参数过大，提高模型的稳定性。

#### **（8） 模型保存与加载**

* 在训练深度学习模型时，需要掌握 **如何保存训练好的模型** 以及 **如何加载模型进行推理或继续训练**。

* 训练好的模型可以用于 **迁移学习、模型部署** 等任务。

比如

```python
#保存模型： 
torch.save(model.state_dict(), "model.pth")
#加载模型： 
model.load_state_dict(torch.load("model.pth"))
```

***

### **第二阶段：深入 PyTorch**

#### **（1） 卷积神经网络（CNN）**

* CNN 是计算机视觉任务中最重要的模型之一，其核心机制包括 **卷积层、池化层、激活函数**。

* 需要理解 **卷积操作的原理**，以及 **如何通过卷积核提取图像特征**。

* 了解经典 CNN 结构，如 **VGG、ResNet**，并理解它们的改进点。

#### **（2） 循环神经网络（RNN）**

* RNN 适用于处理 **序列数据**（如文本、时间序列），其核心思想是通过 **隐藏状态（Hidden State）** 传递信息。

* 需要理解：&#x20;

  * **RNN 处理序列数据的方式**，如何通过时间步（Time Step）传递信息。

  * **LSTM（长短时记忆网络）** 解决长序列依赖问题。

  * **GRU（门控循环单元）** 作为 LSTM 的简化版本，在某些任务上能提供相似性能但计算成本更低。

### 学习参考

* [小土堆](https://www.bilibili.com/video/BV1hE411t7RN/?spm_id_from=333.999.0.0)（基础入门）

* [李沐--动手学深度学习](https://space.bilibili.com/1567748478)（注意的是，不要过度依赖d2l库，因为正常来说不会使用这个的）

* 吴恩达深度学习课程（原理）

* 花书（《Deep Learning》）

* [StatQuest](https://space.bilibili.com/3546620985608836/video)（原理）

* [深入浅出PyTorch](https://datawhalechina.github.io/thorough-pytorch/)

# 自然语言处理（NLP, Natural Language Processing）

**这个章节以后的所有内容其实都是关于NLP的，层级关系是这样的：**

**NLP（自然语言处理）**

&#x20;**&#x20; ├── PLM（预训练语言模型，如 BERT, GPT, T5）**

**&#x20;      ├── LLM（大语言模型，如 GPT-4, LLaMA, ChatGLM）**

* NLP 是最广泛的概念，包含所有涉及文本处理的任务。

* PLM 是 NLP 任务中的核心技术，通过预训练学习通用语言知识，提升下游任务性能。

* LLM 是 PLM 的扩展，通过 超大规模参数+海量数据训练，在没有额外微调的情况下也能完成复杂 NLP 任务。



**自然语言处理（Natural Language Processing, NLP）** 是人工智能（AI）领域的重要分支，旨在让计算机能够理解、生成和处理人类语言。NLP 涉及多个核心任务，如**文本分类、机器翻译、情感分析、命名实体识别、问答系统**等，广泛应用于搜索引擎、智能助手、自动摘要等场景。

1. **要求**

* 重点掌握 NLP 的核心概念和主要模型，尤其是 **词嵌入、Seq2Seq、注意力机制、Transformer** 这些基础内容。

* 逐步掌握 **深度学习在 NLP 任务中的应用**，包括文本分类、机器翻译、文本生成等任务。

- **重点内容**

#### **（1）词嵌入技术**

* 了解 **Word2Vec、GloVe** 等模型，掌握词向量的生成及其应用。

* 重点理解**词向量的计算原理（如 Skip-gram, CBOW）**。

* 仅需了解，不要求深入实现。

#### **（2）Seq2Seq（序列到序列模型）**

* 了解 **如何处理序列到序列的任务**，如 **机器翻译、文本摘要**。

* 了解 **编码器-解码器（Encoder-Decoder）结构**，包括 RNN、LSTM 版本的 Seq2Seq。

* 可尝试基于 PyTorch复现简单的 Seq2Seq 模型。

#### **（3）注意力机制（Attention）**

* 学习 **Attention 机制**，理解其数学公式与实现：&#x20;

  * **Self-Attention 公式**： $${Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) $$

  * **Multi-Head Attention** 的作用及实现。

* 重点理解 **Transformer** 如何利用 Self-Attention。

#### **（4）Transformer 架构**

* 理解 Transformer 结构，明确：&#x20;

  * **Encoder 作用**（提取输入特征）。

  * **Decoder 作用**（生成输出序列）。

* **尝试复现 Attention Layer**（如果能手写一个 Attention Layer，会有更深刻的理解）。

- **学习参考**

* 李沐 NLP 课程

* HuggingFace 官方教程

* GitHub（多阅读 Transformer 相关模型的源码）

* 谷歌学术

# 预训练模型（PLM, Pre-trained Language Model）

#### **1. 介绍/要求**

* **学习目标**：深入理解 **预训练语言模型（PLM, Pre-trained Language Model）**，包括 **BERT、GPT、T5、XLNet** 等，掌握其训练方式和应用场景。

* **重点内容**：&#x20;

  * **自监督学习（Self-Supervised Learning）**：了解 NLP 预训练模型如何通过自监督任务学习表示。

  * **自编码（Autoencoding） vs. 自回归（Autoregressive）**：理解不同的预训练范式及其优缺点。

  * **不同预训练模型的掩码策略**：对比 BERT、GPT、T5、XLNet、UniLM等模型的掩码机制。

  * **Tokenizer 机制**：如何将文本转换为模型可理解的输入。

  * **Transformer 在预训练模型中的应用**：掌握 **Attention、Layer Normalization、Residual Connection** 等关键技术。

***

#### **2. 重点内容**

##### **（1） 自监督学习（Self-Supervised Learning）**

* **什么是自监督学习？**

  * 自监督学习是一种 **无需人工标注** 的学习方式，模型通过自身生成监督信号进行训练。

  * 主要用于 **大规模文本数据预训练**，在无监督数据上学习有效的文本表示。

* **自监督任务分类**：&#x20;

  * **自编码（Autoencoding）任务**（如 BERT）

  * **自回归（Autoregressive）任务**（如 GPT）

  * **对比学习（Contrastive Learning）任务**（如 SimCSE）

##### **（2） 自编码 vs. 自回归**

##### **（3） 不同的预训练模型的掩码策略**

* **BERT（Bidirectional Encoder Representations from Transformers）**

  * **掩码机制**：&#x20;

    * **MLM（Masked Language Model）**：在输入文本中随机选择 15% 的 Token 进行 Mask，模型需要预测被遮挡的 Token。

    * **NSP（Next Sentence Prediction）**：判断两句话是否在原文中相邻（BERT 原始版本使用，但 RoBERTa 取消）。

  * **优点**：双向编码，可用于文本理解任务。

  * **缺点**：不适用于生成任务。

* **GPT（Generative Pre-trained Transformer）**

  * **掩码机制**：&#x20;

    * **Causal Masking（因果遮蔽）**：只能看到之前的 Token，避免模型窥探未来信息。

  * **优点**：适用于生成任务（如对话生成、代码生成）。

  * **缺点**：单向编码，无法建模完整上下文。

* **T5（Text-to-Text Transfer Transformer）**

  * **掩码机制**：&#x20;

    * **Span Masking（片段遮蔽）**：随机遮蔽文本片段，而不是单个 Token，提高模型对文本片段的理解能力。

  * **优点**：适用于 **文本转换任务（翻译、摘要、问答）**。

  * **缺点**：训练成本高。

* **XLNet（Generalized Autoregressive Pretraining）**

  * **掩码机制**：&#x20;

    * **Permutation Language Model（排列语言模型）**：通过随机排列 Token 的顺序，使模型学习更全面的上下文信息。

  * **优点**：兼具 BERT 的双向性和 GPT 的自回归特性。

  * **缺点**：计算开销大，实际应用较少。

##### **（4） Tokenizer 机制**

* **作用**：将自然语言转换为 Token ID 序列，使 Transformer 能够处理文本。

* **常见 Tokenizer 类型**：&#x20;

  * **WordPiece（BERT 使用）**：基于子词单元拆分。

  * **Byte-Pair Encoding（BPE, GPT 使用）**：基于统计规律合并最常见的字符对。

  * **SentencePiece（T5, XLNet 使用）**：无需分词表，适用于多语言任务。

##### **（5） Layer Normalization（LN） vs. Batch Normalization（BN）**

**Batch Normalization（BN）** 是针对 **整个 mini-batch** 进行归一化，即在 **批次维度** 计算均值和方差，对数据进行标准化。BN 依赖 **批次大小**，在不同 batch 之间共享统计信息，因此在小 batch 训练时效果可能不稳定。BN 主要用于 **CNN（卷积神经网络）**，能够加速训练并减少梯度消失。

**Layer Normalization（LN）** 是针对 **单个样本的特征维度** 进行归一化，即在 **特征维度** 计算均值和方差，使得归一化过程独立于 batch 大小。LN 适用于 **NLP（自然语言处理）** 任务，尤其是 **Transformer** 结构，它不受 batch 变化的影响，因此在 **RNN、Transformer** 等序列模型中表现更稳定。

***

#### **3. 学习参考**

* **HuggingFace 官方教程**

* **GitHub（Transformer 相关源码）**

* **Datawhale 开源 NLP 课程**

* **BERT、GPT、T5、XLNet、ELECTRA 论文**

***

### **总结**

* **理解自监督学习（Self-Supervised Learning）如何用于 NLP 预训练**。

* **掌握自编码（BERT）和自回归（GPT）的区别**，理解 Encoder、Decoder 结构。

* **学习不同预训练模型的掩码机制（Masking Strategies）**。

* **理解 Transformer 结构及其在 NLP 任务中的应用**。

### 学习参考

* [huggingface](https://huggingface.co/)

* [开源社区datawhale](https://github.com/datawhalechina)

# 大语言模型（LLM, Large Language Model ）

**大语言模型（LLM）**&#x662F;基于深度学习和Transformer架构的强大自然语言处理工具，能够理解和生成各种类型的文本。它们通过在大规模文本数据上进行预训练，具备强大的语言理解、推理和生成能力。LLM的特点包括大规模的预训练、迁移学习、灵活适应多任务能力、以及出色的文本生成和上下文理解能力。

**部署：**

大模型的显存占用大，学会大模型部署到服务器上，比如autodl。

**使用：**

一般采用的是transformer库和peft库，模型一般在[huggingface](https://huggingface.co/models?library=pytorch)里。

以下是一个简单的模型使用方法:

```python
import transformers
import torch
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
pipeline = transformers.pipeline("text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)
messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]
outputs = pipeline(
    messages,
    max_new_tokens=256,
)
print(outputs[0]["generated_text"][-1])
```

模型的使用相对是比较简单的，难的是对于模型源码和原理的理解

**从经典的Llama开始学习：**

1\. **LoRA（低秩适配）**

* **目标**：理解LoRA技术的原理和应用，学习如何通过低秩矩阵减少大模型的计算和存储需求。

* **学习内容**：

  * **LoRA原理**：深入理解LoRA如何通过引入低秩矩阵来减少需要更新的参数数量，同时保持微调效果。

  * **LoRA与其他微调方法的对比**：了解LoRA与传统微调方法（如全量微调、Adapter、Prompt Tuning等）的优缺点。

2\. **梯度检查点（Gradient Checkpointing）**

* **目标**：掌握梯度检查点技术，通过减少内存占用来提升大模型训练的效率。

* **学习内容**：

  * **梯度检查点概念**：理解梯度检查点的基本原理：通过选择性地保存中间激活值来减少内存占用，而在反向传播时重新计算这些激活。

3\. **RoPE（旋转位置编码）**

* **目标**：理解RoPE技术，掌握其在自注意力机制中的应用，尤其是针对长序列建模的优化。

* **学习内容**：

  * **位置编码的作用**：复习Transformer中的标准位置编码（Sinusoidal Position Encoding）的工作原理和局限性。尤其是和BERT的绝对位置编码的比较。

  * **RoPE原理**：学习RoPE的设计思想：通过旋转矩阵来增强位置编码在模型中的表现，尤其是在处理长序列时如何避免传统位置编码的性能瓶颈。

4\. **QKV缓存（Query-Key-Value 缓存）**

* **目标**：掌握QKV缓存技术，理解其如何加速自注意力计算，特别是在推理阶段。

* **学习内容**：

  * **自注意力机制**：回顾自注意力机制的工作原理，重点理解Query、Key和Value的计算方式。

  * **QKV缓存概念**：学习QKV缓存的作用，即在推理阶段缓存先前层计算的Q、K、V值，避免每次重新计算，从而加速推理过程。





